From: AKASHI Takahiro <takahiro.akashi@linaro.org>
To: private-kwg@linaro.org, alex.bennee@linaro.org
Cc: 
Bcc: takahiro.akashi@linaro.org
Subject: [Weekly] Week ending 2022-09-16
Reply-To: 

Stratos
=======
[STR-68] (network latency behaviour with VMs)
* No update. See [1].

  - mqprio + etf
  - sample-app-taprio[2] with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.18+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2          39.3   46.8   49.8    1.6
  host1-to-vm
    VFIO                  57.0   62.7  100.4    4.0    (NOTE: no netperf)
    macvtap               92.5  139.1  275.4   35.6
    tap                  107.4  228.8  434.4   70.4
    OVS                  212.6  344.0  612.8   66.3
    eBPF (XDP)          5598.0 6586.0 7659.0  390.3

  TODO: investigate eBPF case

  TODO: find out a generic configuration of bridge to fix multicast issue
  (although this won't have any impact on measured results).

  TODO: With my own benchmark program, I sometimes see that read/readmsg()
  after select() fails.

[STR-92] (Setup for TSN)
* I closed this card.
  
[STR-76] Break down the latency
===8<== (from last week's report)
  The time consumed on the receiver side can be split into 4 time spans:
  a) from interrupt arrival to invocation of interrupt handler
  b) from interrupt handler to wake-up of vhost process
  c) vhost processing to wake-up of kvm process
  d) from kvm wake-up to guest wake-up and packet reception by guest's driver 

As far as this trial is concerned, (a)+(b), (c) and (d) are running
on different processors.

  (usec)  w/o netperf    w/ 3 netperf's
  latency  71             150
  (a)      <= 20           <= 62
  (b)      19              25
  (c)      17              31
  (d)      <= 14           25

  NOTE:
  * Those are NOT mean values, but ones observed in a single packet handling.
  * (a) is not accurate because we don't know exactly when the packet arrived.

At a glance, (a)+(b) seems to contribute most to a gap between the two cases.
Based on the ftrace log, major disturbing factors in (a)+(b) are
  1) interrupt handling for netperf packets
  2) interrupt handling for arch timer
===>8===

The function flow in the kernel was roughly illustrated in my doc[1],
page 17 and 18. Please take a look at them first.

So what I wanted to achieve now was to isolate all the code execution related to
the benchmark program from other activities evoked by netperf load.
  a) move vhost process (for guest's virtio net) to CPU#3
  b) forward only interesting interrupts to CPU#3
(Please note the benchmark program/thread is running solely on CPU#3.)

* for (a),
Looking at the ftrace log, the overhead of context switching and IPI to a remote
processor is a relatively small comparing to a measured latency and (a) may not
help improve latency very much.
I actually tried this and observed the latency got *longer*. I don't know why.

* for (b),
I tried to deliver received packets to different rx queues depending on
VLAN priorities and then instructed the kernel to deliver a respective
interrupt to a different CPU.

There seem a few methods to achieve selective queuing/packet distribution:
1) Kernel's RSS(Receive Side Scaling)/RPS(Receive Packet Scaling)/
   RFS(Receive Filter Scaling)
   Those techniques are used for scale the network performance
   on multi-processor systems. But the kernel document[3] said,
   "Multi-queue distribution can also be used for traffic prioritization, but
   that is not the focus of these techniques."

   While RFS, in particular, can improve application's locality, but I don't
   think we can isolate a specific traffic from others with those techniques.
   (Hashing is used for filtering and different traffic can fall into the same
   hash index.)

   (You can also refer [4] for better understanding of RSS.)

2) NIC's hardware offload of filtering
   Based on mac address/ip address/vlan tci and etc., matching packets can be
   selectively delivered to a specific queue.
   This feature can be managed with "ethtool --config-nfc (or config-ntuple)".
   For example,
      ethtool -i enp1s0 -N vlan 2005 vlan-mask 0xe000 action 1
      ethtool -i enp1s0 -N vlan 6005 vlan-mask 0xe000 action 3
      (forwarding packets with VLAN PCP 1 to queue#1, PCP 3 to queue#3)

   Once the queuing is correctly configured, we can distribute NIC-related
   interrupts among CPU's with, so called, irq-affinity, something similar to
   cpu affinity:
      echo 8 > /proc/irq/<irq_no>/smp_affinity (for CPU#3 only)

   In this setup, I don't know how packets in non-VLAN domain, that is netperf
   traffic, are handled and so I changed the network configuration a bit
   and create a new VLAN for netperf and asigned those packets to VLAN PCP 1
   by using
      ip link add link enp1s0 name enp1s0.1 vlan id 1 egress-qos-map 3:3 5:5 ...
      (mapping socket priority 3 to PCP 3 and so on)

   (NOTE: I'm also trying to use "macvlan" instead of "vlan" here so that
   we can use a different mac source address and utilize it for filtering
   packets based on mac address on receiver side as well.)

   At the end,
   - the benchmark program sends a test packet with priority 3 on VLAN#5,
     (while netperf traffic on VLAN#1)
   - it is transferred with PCP 3 over the network, 
   - received/delivered into queue#3 on the receiver side and
   - an associated interrupt will be triggered on CPU#3.

   So the results? I've observed strange behaviors in rx queuing. 
   I'm almost sure I've done the configuration correctly, but some of traffic
   by netperf load was also delivered to queue#3. Obviously packets are
   not properly handled (at hardware level?). I confirmed that all the packets
   are transferred with appropriate PCP value by looking at tcpdump log.

   I have no idea about what's happening here.

=== update (Sep 20) ===
- Using TCP_STEAM (used until now) at netperf,
  IRQs      cpu#0   cpu#1   cpu#2   cpu#3
  TxRx-0:  642031       0       0       0   --+
  TxRx-1:       0  312611       0       0   --+
  TxRx-2:       0       0      61       0     |
  TxRx-3:       0       0       0  312627   --+-- distributed

- Using UDP_STREAM at netperf,
  IRQs      cpu#0   cpu#1   cpu#2   cpu#3
  TxRx-0:     334       0       0       0
  TxRx-1:       0      46       0       0
  TxRx-2:       0       0     116       0
  TxRx-3:       0       0       0  107185  -- I exected queue#0, not #3

=== ===

* When I investigated the issue above, I had to check against NIC's datasheet.
  As far as I searched, I was not able to find out the datasheet for Intel i225
  on the internet and I had no choice other than consulting the datasheet for i210.
  (I hope the difference is a little. i210 supports only up to 1Gbps ether.)

* I don't know yet how we can remove arch-timer interrupts.
   NO_HZ?, but the benchmark program calls select/poll() to wait for
   incoming packets and so we will need a timer on guest VM.


[STR-41] (Implement SVC/HV passthrough for Xen on ARM64)
* No progress

[STR-66]
  No progress

[STR-70] (Check the status of eBPF for ARM64)
* No progress


  [1] https://docs.google.com/presentation/d/1zxxHVKgzMAcgB6_RIJ8i0v6yzqAff6gml7XuS5Xsyk8/edit?usp=sharing
      (This link was updated to revision 2.)
  [2] https://github.com/intel/iotg_tsn_ref_sw/tree/apollolake-i
  [3] kernel's Documentation/networking/scaling.rst
  [4] https://archive.fosdem.org/2021/schedule/event/network_performance_in_kernel/attachments/slides/4433/export/events/attachments/network_performance_in_kernel/slides/4433/chevallier_network_performance_in_the_linux_kernel.pdf

U-Boot support
==============
[TS-219]
* The follow-up patch[5]
  No progress. Although a patch for testing in the original patch series is yet
  to be merged, I think we can close this card.

(MISC)
* Reviewed Sughosh's multibank-fw-update patch (v10)
* Reviewed and made comments[6] Heinrich's efi_driver-related patch
  as Simon gave me a heads-up linking to DM(Driver Model)-UEFI integration.
* Reviewed and made some comments on Simon's block device driver-related patch

* Review of secure boot test spec
  TODO: Yet to review the updated test spec[7].

* Device path creation improvement for USB device[8]
  No progress
  Heinrich posted a small patch to be able to handle a "media change"
  on USB. For me, it seems to be a fragile approach and commented on it[9].
  I believe that enhancing [8] is a much better solution.

[5] https://lists.denx.de/pipermail/u-boot/2022-May/483962.html
[6] https://lists.denx.de/pipermail/u-boot/2022-September/494281.html
    https://lists.denx.de/pipermail/u-boot/2022-September/494399.html
[7] https://github.com/stuyod01/edk2-test/blob/secure-boot/uefi-sct/Doc/UEFI-SCT-Case-Spec/SCT_Secure_Boot.md
[8] https://lists.denx.de/pipermail/u-boot/2021-November/468216.html
[9] https://lists.denx.de/pipermail/u-boot/2022-September/494764.html


=== Patch status ===
    uboot/secboot bugfix: merged in v2022.07-rc
    uboot/short_path v2: partially merged in v2022.07-rc4
    uboot/uefi removable rfc: merged in v2022.07-rc2
    uboot/disk_integ v5: merged in v2022.07-rc1

=== Issues ===
* I still need a PCIE (intel x550 or 82576) card for SR-IOV testing.

=== Travel&Holiday&Event ===
Sep 19: Respect-for-Senior-Citizens Day (public holiday)
Sep 23: Autumnal Equinox Day (public holiday)


