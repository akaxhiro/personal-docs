From takahiro.akashi@linaro.org Fri Aug 26 22:25:06 2022
Date: Fri, 26 Aug 2022 22:25:03 +0900
From: AKASHI Takahiro <takahiro.akashi@linaro.org>
To: private-kwg@linaro.org, alex.bennee@linaro.org
Subject: [Weekly] Week ending 2022-08-26
Message-ID: <20220826132503.GA213311@laputa>

Stratos
=======
[STR-68] (network latency behaviour with VMs)
* Based on my insights at STR-92, I re-ran measurements with new setup.
  See [1] for details of the latest result, in particular, page 12, 13 and 14.

  - mqprio + etf
  - sample-app-taprio[2] with 3 instances of netperf
    (The definition of "latency" is different from the one in my benchmark program.
     See also [1].)

  PCIe NIC (i225) with kernel 5.18+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2          39.3   46.8   49.8    1.6
  host1-to-vm
    VFIO                  57.0   62.7  100.4    4.0    (NOTE: no netperf)
    macvtap               92.5  139.1  275.4   35.6
    tap                    -      -      -       -
    OVS                    -      -      -       -
    eBPF (XDP)             -      -      -       -

* TODO: Ilias raised a concern about the result of eBPF case, saying
  that the latency should be much smaller than other cases.

[STR-92] (Setup for TSN)
* In my last weekly report, I said that "full offload" would be the best
  for *deterministic* periodic transmission of packets.
  It seems that it's not a right choice as, from measurement to measurement,
  I saw different results even with the same configuration (of TSN).

  Then I attempted "mqprio + etf" case. So far it looks promising, at least,
  in my current test environment.
  "mqprio" is quite similar to "taprio", but unlike taprio, it does not provide
  traffic-class-based gate control, instead, its scheduling policy relies on
  the underlining hardware(NIC)'s QoS. On Intel i225 platform, the NIC has
  4 tx queues and queue#0 is by default at the highest priority, #3 is
  at the lowest.  So packets sent through tx queue#0 are almost always expected
  to be prioritized.

  My preliminary measurements with this configuration marked good performance,
  with constant low latency, even when we had network stress by multiple netperf
  instances. I'd like to use this configuration in my subsequent experiments,
  at least, for initial analysis.

* Vincent hinted that the configuration above might not be able to be applied to
  *all* the cases, especially, if there are more than one applications which require
  some level of bandwidth and latency for their network streams.

  Currently, it is beyond my scope, or my current assumption described in [1].
  If those cases really matter, we will have to review/revise what are the real
  requirements in our efforts.
	-> Alex?

[STR-76] Break down the latency
* To investigate what's happening on receiver side (either host or kvm),
  I'm thinking of using tcpdump/ftrace.

* First I tried to trace incoming packets with tcpdump.
  If we can capture ingress (macvlan interface) and egress(tap interface) on host,
  remember macvtap is a combination of macvlan and tap, we will be able to
  follow timelines about how and when packets are handled inside the host.

  Unfortunately, this attempt was not successful because trace data captured for
  macvlan and data captured for tap were *identical*, the same packets with
  the same timestamps, sigh.
  (Probably, pcap is set to dump packets at the same location in network stack.)

* For ftrace, I'm looking into the kernel code, which kernel functions should
  be traced. Some in network stack, some in context switch and others in kvm.

  I have a couple of concerns in using ftrace, one is that a timestamp recorded in
  ftrace is a "delta", or relative time within a captured log.
  If we cannot know absolute times (CLOCK_TAI) against events/functions, it can be
  difficult to match ftrace log with the output from the benchmark program and
  find out/identify a specific sequence of code which handles a particular packet
  (from the program). 

* TODO: On my own benchmark program, I sometimes see that read/readmsg()
  after select() fails.

[STR-41] (Implement SVC/HV passthrough for Xen on ARM64)
* No progress

[STR-66]
  No progress

[STR-70] (Check the status of eBPF for ARM64)
* No progress


  [1] https://docs.google.com/presentation/d/1zxxHVKgzMAcgB6_RIJ8i0v6yzqAff6gml7XuS5Xsyk8/edit?usp=sharing
      (This link was updated to revision 2.)
  [2] https://github.com/intel/iotg_tsn_ref_sw/tree/apollolake-i
  [3] https://docs.starlingx.io/developer_resources/stx_tsn_in_kata.html


U-Boot support
==============
[TS-219]
* The follow-up patch[4]
  No progress. Although a patch for testing in the original patch series is yet
  to be merged, I think we can close this card.

(MISC)
* Reviewed/tested Kojima-san's eficonfig patch
* Review of secure boot test spec
  TODO: Yet to review the updated test spec[5].

* Device path creation improvement for USB device[6]
  No progress

[4] https://lists.denx.de/pipermail/u-boot/2022-May/483962.html
[5] https://github.com/stuyod01/edk2-test/blob/secure-boot/uefi-sct/Doc/UEFI-SCT-Case-Spec/SCT_Secure_Boot.md
[6] https://lists.denx.de/pipermail/u-boot/2021-November/468216.html


=== Patch status ===
    uboot/secboot bugfix: merged in v2022.07-rc
    uboot/short_path v2: partially merged in v2022.07-rc4
    uboot/uefi removable rfc: merged in v2022.07-rc2
    uboot/disk_integ v5: merged in v2022.07-rc1

=== Issues ===
* I still need a PCIE (intel x550 or 82576) card for SR-IOV testing.

=== Travel&Holiday&Event ===
Sep 19: Respect-for-Senior-Citizens Day (public holiday)
Sep 23: Autumnal Equinox Day (public holiday)

