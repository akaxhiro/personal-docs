From: AKASHI Takahiro <takahiro.akashi@linaro.org>
To: private-kwg@linaro.org, alex.bennee@linaro.org
Cc: 
Bcc: takahiro.akashi@linaro.org
Subject: [Weekly] Week ending 2022-11-18
Reply-To: 

Stratos
=======
[STR-76] Break down the latency
* investigated the detailed control path for various network configurations.
  See [1], page 30-36.

  I, Alex and Vicent have talked about the goal and approach of further
  investigation in my experiments.
  I don't think the concrete steps are very clear but we somehow agreed
  that we need to mention the deterministic aspects in the control path
  for network packet delivery. To do so, we should break down the path
  into several stages and verify/guarantee that each stage will be deterministic
  or in other words, the maximum time to be consumed in each stage will be predictable.

  As the first step, I investigated the control path,
    - from the entry to irq handler (top-half or hard-irq) to wake-up of ksoftirqd
    - from softirq (bottom-half) to network stack, wake-up of vhost
    - from vhost to wake-up of guest VM (kvm process)
  by using ftrace logs.

  Well, I could have detailed understandings of the control path, but
  actually there are bunch of conditions & branches in the middle of the flow.
  Even with the knowledge I now have, I don't think it's easy to *theoretically
  prove* that the path is deterministic.

* trying to switch over to PREEMPT_RT kernel.

  --- preliminary Nov 21 ---
  tap+bridge, taprio(full), PREEMPT_RT kernel
                         min    avg    max    stddev
  host1-to-vm
    no netperf           55.6   64.8  146.2    6.7
    3 netperf           117.6  683.0 2717.2  436.0
    +rx filter/irq aff  136.0  176.7  259.2   15.8

        netperf -> queue#0/1 -> cpu#0/1
        bench   -> queue#2   -> cpu#2
        vhost -> cpu#2
        kvm   -> cpu#3
  --------------

  I applied the remaining RT patch from kernel.org site to Debian's kernel source
  code (v5.19) and built the kernel.
  Interestingly, Debian's kernel configuration for RT build misses
  several kernel features, including
    - CONFIG_KVM
    - CONFIG_DISABLE_EFI_RUNTIME_DEFAULT (=y)
    - CONFIG_TRANSPARENT_HUGEPAGE
    - CONFIG_NET_RX_BUSY_POLL
    - CONFIG_INLINE_[SPIN|READ|WRITE]_LOCK[_BH|_IRQ]

  I guess that some may cause unexpected (asynchronous) intercepts in the middle
  of kernel code execution and others may require unpredictable amount of time
  to complete the operations.
  In particular, I have a concern on KVM.
  I wonder if there is any possibility that the host OS or guest VM(s) might
  be unexpectedly suspended due to some KVM operations *across vcpus*, i.e.
  smp_call_function_xxx(). Actually there found are a couple of callsites
  in kvm code,
  including
    - page/cache? invalidation
    - VM reset/halt operations

  Any thoughts?
        -> Alex

[LBO-293] (Talk about the Stratos work on characterising the impact of VMs ...)
* I'm not sure if Ebba has reviewed my draft[2] or not.

[STR-68] (network latency behaviour with VMs)
* No update.
  See [1] for detailed results.

  - mqprio + etf
  - sample-app-taprio with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.18+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2          39.3   46.8   49.8    1.6
  host1-to-vm
    VFIO                  57.0   62.7  100.4    4.0    (NOTE: no netperf)
    macvtap               92.5  139.1  275.4   35.6
    tap                  107.4  228.8  434.4   70.4
    OVS                  212.6  344.0  612.8   66.3
    eBPF (XDP)          5598.0 6586.0 7659.0  390.3

  - taprio (full offload)
  - sample-app-taprio with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.19+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2       
  host1-to-vm
    VFIO               
    macvtap            
    tap                   96.7  124.6  390.0   17.9
    OVS                
    eBPF (XDP)         
    OVS+AF_XDP            92.1  279.0 1669.4  161.1

  TODO: tune OVS+AF_XDP case

  TODO: find out a generic configuration of bridge to fix multicast issue
  (although this won't have any impact on measured results).

  TODO: With my own benchmark program, I sometimes see that read/readmsg()
  after select() fails.

[STR-41] (Implement SVC/HV passthrough for Xen on ARM64)
* No progress

[STR-66]
  No progress

[STR-70] (Check the status of eBPF for ARM64)
* No progress

  [1] https://docs.google.com/presentation/d/1zxxHVKgzMAcgB6_RIJ8i0v6yzqAff6gml7XuS5Xsyk8/edit?usp=sharing
  [2] https://docs.google.com/document/d/1ZBFCJTgsS-aUsyxofhH1pGrH6xlNaFG4JkVMOmZOv1s/edit?usp=sharing
      https://docs.google.com/document/d/1lNS1cZb5A3gvDvz8c7RHJgxPpGpRNFTg/edit?usp=sharing&ouid=113992592841977909554&rtpof=true&sd=true (docx)


U-Boot support
==============
No progress

[TS-219]
* The follow-up patch[3]
  No progress. Although a patch for testing in the original patch series is yet
  to be merged, I think we can close this card.

(MISC)
* Device path creation improvement for USB device[4]
  No progress (This topic might be related to UEFI-DM integration).

[3] https://lists.denx.de/pipermail/u-boot/2022-May/483962.html
[4] https://lists.denx.de/pipermail/u-boot/2021-November/468216.html


=== Patch status ===
    uboot/SETUP_EARLY v1: submitted on 10/21
    uboot/short_path v2: partially merged in v2022.07-rc4

=== Issues ===
* I still need a PCIE (intel x550 or 82576) card for SR-IOV testing.

=== Travel&Holiday&Event ===
Nov 23: Labor Thanksgiving Day (public holiday)
