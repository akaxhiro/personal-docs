From: AKASHI Takahiro <takahiro.akashi@linaro.org>
To: private-kwg@linaro.org, alex.bennee@linaro.org
Cc: 
Bcc: takahiro.akashi@linaro.org
Subject: [Weekly] Week ending 2022-11-25
Reply-To: 

Stratos
=======
[LBO-293] (Talk about the Stratos work on characterising the impact of VMs ...)
* Updated the draft[2] after Andrea's and Ebba's comments (mostly grammatical
  ones).

--- Nov. 28 ---
When we talk about the analysis of latencies, we need to pay attention to
two aspects/objectives:
  a. shorten/reduce the execution path in packet handling
  b. ensure the determinism
(Can easily confuse them.)
---------------

[STR-68] (network latency behaviour with VMs)
* Before performing re-measurements for all the cases, I wanted to investigate
  a couple of issues that I have seen:
  - switch to PREEMPT_RT kernel (on the host, not guest VM)
  - latencies in case of tap+XDP
  - tunings in case of OVS+AF_XDP

* switch to PREEMPT_RT
  I'm still wondering why CONFIG_KVM is disabled in Debian's configuration.
        -> Alex?

  Here is a preliminary result of the measurement.
  tap+bridge, taprio(full), PREEMPT_RT kernel

                         min    avg    max    stddev
  host1-to-vm
    no netperf           55.6   64.8  146.2    6.7
    3 netperf           117.6  683.0 2717.2  436.0
    +rx filter/irq aff  136.0  176.7  259.2   15.8

No RT kernel
    tap                   96.7  124.6  390.0   17.9

  To get better results, I needed to use filtering/irq affinity properly.
        --------------
        netperf -> queue#0/1 -> cpu#0/1
        bench   -> queue#2   -> cpu#2
        vhost -> cpu#2
        kvm   -> cpu#3
        --------------

  Vincent asked me to check whether cpufreq and cpuidle are used or not.
  The mainline kernel doesn't enable the both on MacchiatoBin. 
  I also tried the additional flags of "isolcpus", nohz, domain and
  manage_irq but they seem to have any visible impacts in my experiments.

  On PREEMPT_RT kernel, all the interrupts are handled (TOP-half?) in dedicated
  irq threads. So I also tried to bind threads for different rx queues to
  different cpus, but saw no difference.
  Configuring ksoftirqd threads seems more crucial.

* latencies in tap+XDP
  I struggled again to set up this configuration; the network doesn't work.
  --- Nov 28 ---
  Fixed. it was due that I mistakenly cleaned up the XDP programs some time ago.

  Executing a XDP program seems to be costly.
  Invoking it at every packet received will easily consume a budget
  (or time_limit) allowed for napi_poll().
	-> simplify the XDP program to hard-code the routing
  (I'm not sure, though, why the routing in the standard "bridge" doesn't matter.)

  Hence a fewer packets will be handled at a time in softirq.
  --------------

* tunings in OVS+AF_XDP
  In my previous experiment, I dedicated one rx queue solely to PTP packets.
  I'm wondering if we can configure "flow" rules of OVS (or more precisely,
  OpenFlow) to do the same thing without consuming one rx queue.

  Even if we can, however, this configuration might be "de-tuning" for
  PTP as it is nothing but an uncertain overhead to the latency calculation
  in clock synchronization process.

* No update.
  See [1] for detailed results.

  - mqprio + etf
  - sample-app-taprio with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.18+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2          39.3   46.8   49.8    1.6
  host1-to-vm
    VFIO                  57.0   62.7  100.4    4.0    (NOTE: no netperf)
    macvtap               92.5  139.1  275.4   35.6
    tap                  107.4  228.8  434.4   70.4
    OVS                  212.6  344.0  612.8   66.3
    eBPF (XDP)          5598.0 6586.0 7659.0  390.3

  - taprio (full offload)
  - sample-app-taprio with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.19+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2       
  host1-to-vm
    VFIO               
    macvtap            
    tap                   96.7  124.6  390.0   17.9
    OVS                
    eBPF (XDP)         
    OVS+AF_XDP            92.1  279.0 1669.4  161.1

[STR-76] Break down the latency
* No progress

[STR-41] (Implement SVC/HV passthrough for Xen on ARM64)
* No progress

[STR-66]
  No progress

[STR-70] (Check the status of eBPF for ARM64)
* No progress

  [1] https://docs.google.com/presentation/d/1zxxHVKgzMAcgB6_RIJ8i0v6yzqAff6gml7XuS5Xsyk8/edit?usp=sharing
  [2] https://docs.google.com/document/d/1ZBFCJTgsS-aUsyxofhH1pGrH6xlNaFG4JkVMOmZOv1s/edit?usp=sharing
      https://docs.google.com/document/d/1lNS1cZb5A3gvDvz8c7RHJgxPpGpRNFTg/edit?usp=sharing&ouid=113992592841977909554&rtpof=true&sd=true (docx)


U-Boot support
==============
No progress

[TS-219]
* The follow-up patch[3]
  No progress. Although a patch for testing in the original patch series is yet
  to be merged, I think we can close this card.

(MISC)
* Device path creation improvement for USB device[4]
  No progress (This topic might be related to UEFI-DM integration).

[3] https://lists.denx.de/pipermail/u-boot/2022-May/483962.html
[4] https://lists.denx.de/pipermail/u-boot/2021-November/468216.html


=== Patch status ===
    uboot/SETUP_EARLY v1: submitted on 10/21
    uboot/short_path v2: partially merged in v2022.07-rc4

=== Issues ===
* I still need a PCIE (intel x550 or 82576) card for SR-IOV testing.

=== Travel&Holiday&Event ===
Dec 29-: New year holidays
