From: AKASHI Takahiro <takahiro.akashi@linaro.org>
To: private-kwg@linaro.org, alex.bennee@linaro.org
Cc: 
Bcc: takahiro.akashi@linaro.org
Subject: [Weekly] Week ending 2022-11-11
Reply-To: 

Stratos
=======
# Once Stratos project goes soft-shutdown, where does this task
# belong to?

[STR-68] (network latency behaviour with VMs)
* continued to try OVS with AF_XDP.
  I reviewed a configuration, especially regarding hugepages,
  and modified kvm options. Then the network got started.

    $ sysctl -w vm.nr_hugepages=800
    $ kvm ... -m 1G ... \
          -object memory-backend-file,id=mem,size=1024M,mem-path=/dev/hugepages,share=on \
          -numa node,memdev=mem -mem-prealloc"

    (memory-backend-file size must be equal to the size of main memory.)

* But there was another problem; PTP didn't work as the host didn't
  receive a clock master announcement message from the remote host.
  I asked Ilias for help and found out a workaround.

  Normally, OVS spawns a PMD (Polling Mode Driver) thread which keeps
  polling on AF_XDP sockets. When we create a AF_XDP port (or netdev in
  OVS terminology), AF_XDP sockets are opened *per rx queues* of NIC.
  So, for example, by doing the following steps,
    - configure a NIC with *three* rx (actually combined) queues 
         $ ethtool -L enp1s0 combined 3
    - create and add a AF_XDP netdev with n_rxq=2
      (then, OVS polls the first *two* rx queues only)
         $ ovs-vsctl add-port br0 enp1s0 -- set interface enp1s0 \
               type="afxdp" option:n_rxq=2
    - use a NIC filter function to dispatch all PTP packets (identified
      by a dedicated multicast address) to queue#3
         $ ethtool -N enp1s0 flow-type ether dst 01:1b:19:00:00:00 \
               m 00:00:00:00:00:00 action 2

  then (I believe that) PTP packets are handled not by OVS but by
  the kernel network stack and the clock will start synchronized
  on the host.

  Since PTP works now, I can measure the latency in OVS+AF_XDP case
  in the same way as in the other cases.

  I'm not sure, however, that this is a normal (and acceptable) way
  of OVS configuration.
  Another possible solution would be to use a dedicated XDP program
  which precludes the redirection of PTP packets.
  This may require a customization of OVS.

* Added a tentative result of OVS+AF_XDP.
  See [1] for histograms (page 26-28).

  (no update)
  - mqprio + etf
  - sample-app-taprio[2] with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.18+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2          39.3   46.8   49.8    1.6
  host1-to-vm
    VFIO                  57.0   62.7  100.4    4.0    (NOTE: no netperf)
    macvtap               92.5  139.1  275.4   35.6
    tap                  107.4  228.8  434.4   70.4
    OVS                  212.6  344.0  612.8   66.3
    eBPF (XDP)          5598.0 6586.0 7659.0  390.3


  - taprio (full offload)
  - sample-app-taprio with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.19+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2       
  host1-to-vm
    VFIO               
    macvtap            
    tap                   96.7  124.6  390.0   17.9
    OVS                
    eBPF (XDP)         
    OVS+AF_XDP            92.1  279.0 1669.4  161.1

  Please note that the latency under no background traffic(netperf) is
  smaller than tap case (See [1]).
  Given that OVS is basically a PMD thread, there are bunch of room of
  tunings to improve the latency utilizing CPU/IRQ affinity.

  TODO: find out a generic configuration of bridge to fix multicast issue
  (although this won't have any impact on measured results).

  TODO: With my own benchmark program, I sometimes see that read/readmsg()
  after select() fails.

[LBO-293] (Talk about the Stratos work on characterising the impact of VMs ...)
* Notified Ebba of a new revision[2].

From: AKASHI Takahiro <takahiro.akashi@linaro.org>
To: private-kwg@linaro.org, alex.bennee@linaro.org
Cc: 
Bcc: 
Subject: [Weekly] Week ending 2022-11-04
Reply-To: 

Stratos
=======
[STR-68] (network latency behaviour with VMs)
* Attempted OVS with AF_XDP.
  (somehow strangely), we need to enable DPDK as well to enable AF_XDP
  support on OVS.
  Although I successfully built OVS (the latest, 3.0.1) and set it up on
  MacchiatoBin with seeing any error, the network didn't work.
  (Even ping didn't pass.)
  I will keep investigating the issue.

* No update on the result. See [1].

  - mqprio + etf
  - sample-app-taprio[2] with 3 instances of netperf

  PCIe NIC (i225) with kernel 5.18+
  ---------------------------------
                         min    avg    max    stddev
  host1-to-host2          39.3   46.8   49.8    1.6
  host1-to-vm
    VFIO                  57.0   62.7  100.4    4.0    (NOTE: no netperf)
    macvtap               92.5  139.1  275.4   35.6
    tap                  107.4  228.8  434.4   70.4
    OVS                  212.6  344.0  612.8   66.3
    eBPF (XDP)          5598.0 6586.0 7659.0  390.3


  TODO: find out a generic configuration of bridge to fix multicast issue
  (although this won't have any impact on measured results).

  TODO: With my own benchmark program, I sometimes see that read/readmsg()
  after select() fails.

[LBO-293] (Talk about the Stratos work on characterising the impact of VMs ...)
* The draft was a bit revised and it has a new revision[2].

[STR-76] Break down the latency
* No progress

[STR-41] (Implement SVC/HV passthrough for Xen on ARM64)
* No progress

[STR-66]
  No progress

[STR-70] (Check the status of eBPF for ARM64)
* No progress

  [1] https://docs.google.com/presentation/d/1zxxHVKgzMAcgB6_RIJ8i0v6yzqAff6gml7XuS5Xsyk8/edit?usp=sharing
  [2] https://docs.google.com/document/d/1ZBFCJTgsS-aUsyxofhH1pGrH6xlNaFG4JkVMOmZOv1s/edit?usp=sharing
      https://docs.google.com/document/d/1lNS1cZb5A3gvDvz8c7RHJgxPpGpRNFTg/edit?usp=sharing&ouid=113992592841977909554&rtpof=true&sd=true (docx)

U-Boot support
==============
No progress

[TS-219]
* The follow-up patch[3]
  No progress. Although a patch for testing in the original patch series is yet
  to be merged, I think we can close this card.

(MISC)
* Device path creation improvement for USB device[4]
  No progress (This topic might be related to UEFI-DM integration).

[3] https://lists.denx.de/pipermail/u-boot/2022-May/483962.html
[4] https://lists.denx.de/pipermail/u-boot/2021-November/468216.html


=== Patch status ===
    uboot/SETUP_EARLY v1: submitted on 10/21
    uboot/short_path v2: partially merged in v2022.07-rc4

=== Issues ===
* I still need a PCIE (intel x550 or 82576) card for SR-IOV testing.

=== Travel&Holiday&Event ===
Nov 23: Labor Thanksgiving Day (public holiday)
[STR-76] Break down the latency
* No progress

[STR-41] (Implement SVC/HV passthrough for Xen on ARM64)
* No progress

[STR-66]
  No progress

[STR-70] (Check the status of eBPF for ARM64)
* No progress

  [1] https://docs.google.com/presentation/d/1zxxHVKgzMAcgB6_RIJ8i0v6yzqAff6gml7XuS5Xsyk8/edit?usp=sharing
  [2] https://docs.google.com/document/d/1ZBFCJTgsS-aUsyxofhH1pGrH6xlNaFG4JkVMOmZOv1s/edit?usp=sharing
      https://docs.google.com/document/d/1lNS1cZb5A3gvDvz8c7RHJgxPpGpRNFTg/edit?usp=sharing&ouid=113992592841977909554&rtpof=true&sd=true (docx)

U-Boot support
==============
No progress

[TS-219]
* The follow-up patch[3]
  No progress. Although a patch for testing in the original patch series is yet
  to be merged, I think we can close this card.

(MISC)
* Device path creation improvement for USB device[4]
  No progress (This topic might be related to UEFI-DM integration).

[3] https://lists.denx.de/pipermail/u-boot/2022-May/483962.html
[4] https://lists.denx.de/pipermail/u-boot/2021-November/468216.html


=== Patch status ===
    uboot/SETUP_EARLY v1: submitted on 10/21
    uboot/short_path v2: partially merged in v2022.07-rc4

=== Issues ===
* I still need a PCIE (intel x550 or 82576) card for SR-IOV testing.

=== Travel&Holiday&Event ===
Nov 23: Labor Thanksgiving Day (public holiday)
